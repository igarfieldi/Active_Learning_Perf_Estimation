\ifgerman{\chapter{Vorgeschlagene Methoden}}{\chapter{Proposed Methods}}
\label{methods}
As described in the previous section, current performance estimation methods can be roughly categorized into two sections. One group uses information present for the current state of a classifier, e.g. the cross-validation methods. The second, much smaller group takes a look at the performance development over time, up to the current iteration. This encompasses the fitting of function models expected to be of similar shape to the learning curve using already present performance estimates to extrapolate them to future iterations. Typical methods for the estimation of the already witnessed iterations include holdout testing and k-fold cross-validation \cite{FigueroaEtal2012}.

The methods proposed in this work focus on combining both groups in an attempt to use as much information as possible to increase the prediction quality; .632 bootstrapping and the likes focus on the set of available instances, ignoring the process that led to this state. In turn, curve fitting currently makes use of either subpar techniques to obtain the accuracies for each iteration or uses holdout samples which can be costly or not available at all.

To serve as an illustration of the thought process leading to the following techniques, we assume, without loss of generality, a dataset $D = {(\vec{x_i}, y_i)}$ with two class labels $y_i \in {0, 1}$ and $|D| = n$. From this set an active learner selects $k \leq n$ instances which serve as the training set $X_T$ of a classifier $c: \vec{x} \mapsto y$. We would like to know the prediction accuracy of $c$ for the set $D$.

\section{Performance estimation on training sub-sets}
To reach this state, there are several paths available; each path being the sequence in which the active learner draws the selected instances. Assuming $X_T = \{\vec{x}_1, \vec{x}_2, \vec{x}_3\}, 

\textit{Leave-one-out cross validation} is one of the more popular, and at the same time most basic, techniques for performance estimation. Its main advantages are relatively low computational effort and implementation complexity. Additionally, \textit{LOO} for $n$ training instances produces an unbiased estimate for a classifier trained with $n-1$ instances \cite{RodriguezEtAl2013}. 

\todots