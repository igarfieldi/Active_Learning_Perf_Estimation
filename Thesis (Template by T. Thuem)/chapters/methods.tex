\ifgerman{\chapter{Vorgeschlagene Methoden}}{\chapter{Proposed Methods}}
\label{methods}
As described in the previous section, current performance estimation methods can be roughly categorized into two sections. One group uses information present for the current state of a classifier, e.g. the cross-validation methods. The second, much smaller group takes a look at the performance development over time, up to the current iteration. This encompasses the fitting of function models expected to be of similar shape to the learning curve using already present performance estimates to extrapolate them to future iterations. Typical methods for the estimation of the already witnessed iterations include holdout testing and k-fold cross-validation \cite{FigueroaEtal2012}.

The methods proposed in this work focus on combining both groups in an attempt to use as much information as possible to increase the prediction quality; .632 bootstrapping and the likes focus on the set of available instances, ignoring the process that led to this state. In turn, curve fitting currently makes use of either subpar techniques to obtain the accuracies for each iteration or uses holdout samples which can be costly or not available at all.

To serve as an illustration of the thought process leading to the following techniques, we assume, without loss of generality, a dataset $D = {(\vec{x_i}, y_i)}$ with two class labels $y_i \in {0, 1}$ and $|D| = n$. From this set an active learner selects $k \leq n$ instances which serve as the training set $X_T$ of a classifier $c: \vec{x} \mapsto y$. We would like to know the prediction accuracy of $c$ for the set $D$.

\section{Performance estimation on training sub-sets}
The simplest way to obtain performances estimates for the process of instance selection would be to use \textit{leave-x-out(LXO) cross-validation}, with $x \in \{1,...,k-1\}$. This way, we obtain $k \choose x$ subsets of size $x$ from the original training set as well as corresponding test sets of size $k-x$, which enable the performance estimation. Both low computational cost for the individual estimates as well as unbiasedness \cite{RodriguezEtAl2013} are advantages of using \textit{LXO}. While the computational effort for each subset is low, the total amount of possible subsets is $2^k - 2$, resulting in exponential complexity if used natively. Thus, some kind of sampling to reduce the complexity seems desirable.

Another option with regard to the estimation is \textit{bootstrapping}. It offers a little more variety, for different types \textit{na\"{i}ve}, \textit{leave-one-out} and \textit{.632} come to mind. This comes at a price, however; additional computational effort for the creation and testing of the bootstrap samples is necessary. Also, it isn't trivial how to create and handle the subsets. One could proceed similar to its cross-validation counterpart, sampling from the training set without replacement and using that subset as base for the bootstrap. This has two obvious downsides: for one, we are unable to obtain performance estimates for set sizes of one (except for na\"{i}ve bootstrap, which doesn't require that test instances are not within the bootstrap sample). Also, the instances not selected for the subset could be used as (additional) test sets, as they are with cross-validation. However, the influence this would have on the .632 family is unclear; although theoretically additional test instances should not revoke the necessity of weighting between test and bootstrap performance, the weights themselves may be altered, as the bootstrap would be tested on data not available to the training performance estimation. Of course, a possibility could be to also test the training performance on those additional instances, but it wouldn't be the training error anymore.

\subsection{Sub-sampling of fitting points}
Regardless of which estimation technique is used, the complexity still scales exponentially with the set size $k$. To reduce the amount of estimates for the fitting process, some sort of selection has to occur. In this work, three sub-sampling strategies were explored. Reducing the information available naturally has some drawbacks, including an expected higher variance and, if done improper, an added bias. Also, the sampling may influence the fitting itself, potentially inflicting additional penalties to the robustness.

A simple, yet effective method is to cap the number of estimates. Possible options are to either impose a fixed, hard cap for all sizes, or to use a polynomial dependent on $k$, e.g. $k^2$. A potential pitfall is the selection of the subsets $S \subseteq X_T$ to be evaluated. Selecting either randomly over all possible subsets or from pools for each subset size $|S_T| \in \{1,...,k-1\}$ with sizes proportional to $|S_T| \choose k$ prevents unintentional importance assignment to subset sizes.

A related approach exports the computational cost to the fitting process. Instead of selecting multiple subsets per size once and using them for fitting, we select only one subset per size multiple times and fit over them separately. Formally, we have a set $\tilde{S_j} = \{S_1,...,S_{k-1}\}$ with $S_i \subseteq X_T$, $|S_i| = i$ and $j = \{1,...,r\}$. The $S_j$ can be drawn with our without replacement, although the latter may lead to a lower variance, as seeing the same constellation multiple times does not add information, whereas a different one does. The parameter $r$ is up to choosing, with a proposed upper limit of $\prod_{i=1}^{k-1} i \choose k$ for drawing without replacement, which would a much higher complexity than exponential. However, accounting for all possible subset combinations is not necessary, as there are far less unique combinations of accuracy estimations. This is TODO bedingt?? due to the number of test instances available for a given training subset. For example, a classifier trained on a set of size one tested against a set of size three will have four potential test outcomes: either one, two, three or none instances were correctly classified, resulting in an estimated accuracy of $\frac{1}{3}, \frac{2}{3}, 1$ and $0$, respectively. As a grow in size of the training set in turn causes a reduction in size of the test set, the amount of potential outcomes shrinks from $k$ to $2$ for $|S_T| = \{1, ..., k-1\}$. Thus, the number of unique combinations would shrink to $k!$.

Unfortunately, in order to use the estimations as fitting data, they would have to be computed; we need to know the distribution of accuracies for each subset size to weight them properly. Otherwise, a uniform weighting (or randomly picking one) would simply lead to a prediction of 0.5 and we would gain nothing. Thus, using the estimates themselves is not a feasible option for complexity reduction

However, multiple options remain regarding the usage of the individual estimates. Originally, leave-one-out cross-validation uses the arithmetic mean to obtain the final value. In our case, the estimates for each iteration could be averaged, resulting in $k-1$ data points. Another possibility would be to simply pick one estimate for each iteration fitting a curve for them. As this drastically reduces the amount of estimations used, it would be wise to 

To be able to use the training process in the estimation, we make the assumption that the active learner works iteratively, regardless if it actually does. In each iteration, one instance is added to the training set $\tilde{X}_T$, starting at size zero and stopping at size $k-1$. The order of the instance selection does not have to match that of $X_T$; this allows for the simulation of not one, but $k!$ selection processes, as it corresponds to an urn model, sampling instances without replacement.

\textit{Leave-one-out cross validation} is one of the more popular, and at the same time most basic, techniques for performance estimation. Its main advantages are relatively low computational effort and implementation complexity. Additionally, \textit{LOO} for $n$ training instances produces an unbiased estimate for a classifier trained with $n-1$ instances \cite{RodriguezEtAl2013}. 