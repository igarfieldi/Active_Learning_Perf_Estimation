\ifgerman{\chapter{Grundlagen und verwandte Arbeit}}{\chapter{Background and Related Work}}
\label{background}

%- Allgemeine Wissensgrundlagen des Fachgebiets
%- Spezielle Grundlagen, die für das Verständnis erforderlich sind
%- Rahmenbedingungen für die Arbeit
%- Ausführungen zum Stand des Wissens / der Technik
%Als Leitprinzip gilt: Nur Informationen erwähnen, die
%- später benötigt werden,
%- notwendig sind, um die Arbeit oder ihre Motivation zu verstehen
%Das heißt insbesondere,
%- keine Inhalte aus Lehrbüchern, außer
%- diese werden benötigt, um Problemstellung oder Lösungsweg zu definieren.

The main focus of our work is the estimation of classifier performance in the special case of an active learning scenario. To help better understand the problem setting, we will give a brief overview of the concepts of active learning as well as an in-depth look of existing methods for said estimation. We also establish parts of the notation that will be used for the remainder of this work.

\section{Classification and Active Learning}
Many problems to be solved encompass the differentiation between certain "classes" to which their inputs can be assigned. To explain the concept of \textbf{classification} we assume the example of a thermostat. Its purpose is to monitor the temperature in a certain area and fire up a heating unit to raise the temperature if necessary. But for that to happen, the thermostat has to \textit{classify} the measured temperature in the categories "too low" and "warm enough". In this special case a simple threshold usually suffices. Generally, however, a \textit{classifier} $C$ is defined as a mapping of a \textit{feature vector}, also called an \textit{instance}, to its corresponding \textit{class label} $y \in Y = \{y_1, ..., y_m\}$. It is convenient to describe said vector as an n-tuple of \textit{feature values}: $\vec{x} = (f_{a_1,1}, ..., f_{a_n, n})$ with $f_{i,j} \in F_j$ $\forall i = \{1, ..., |F_j|\}, j = \{1, ..., n\}$ and $F_j$ as a \textit{feature}. The goal of a classifier is to approximate the underlying "true" association of a feature vector to its class label $f(\vec{x}) = y$, which can be written as
\begin{equation}
C: F_1 \times ... \times F_n \rightarrow Y
\end{equation}
or $\hat{f}(\vec{x}) = \hat{y}$\cite{RodriguezEtAl2013}. This definition makes it clear that classification is not restricted to single-variable problems. To stick with our example of a thermostat, you may want to consider the humidity, temperature outdoors or whether any windows are open to decide if heating is necessary. Closely related are \textit{regression problems}. While a classifier works with discrete class labels, regression uses a continuous output space.

To obtain a classifier for a specific problem, one makes use of what is called a \textit{learning algorithm} $A$. In a process called \textit{training} a set of instances, the \textit{training data} $X_T = \{(\vec{x}_1, y_1), ..., (\vec{x}_n, y_n)\}$, is taken and such a mapping $C$ using an optimization criterion is created. The training set is a subset of all expected data and should be sampled independently and identically distributed. The whole data set can be modeled as a bivariate distribution with the feature vectors and class labels as random variables: $(\mathbf{X}, \mathbf{Y})$ \cite{RodriguezEtAl2013}. While all learning algorithms are being fed feature vectors to create a classifier, they can be separated into three categories, depending on their requirement for labeling:
\begin{itemize}
\item \textbf{Supervised:}
This type of algorithm requires all of their input data to be labeled as well as a list of all possible class labels. While it may seem like the best option, its drawbacks include the potential cost of the labeling. If the correct class labels are not inherently known, usually a human is required to assign the correct labels manually. With unlabeled data readily available for problems like speech recognition, this is a very expensive part of the process. Another issue is the propagation of errors; any mistake made during labeling is carried over into the modeling of the classifier.

\item \textbf{Unsupervised:}
Instead of requiring all data to be labeled, unsupervised algorithms ignore class labels completely. This saves the cost of labeling the data, but many algorithms require some sort of tuning parameters (e.g. the cardinality of $Y$ and shape of underlying probability distribution). It is similar to the process of density estimation from statistics.

\item \textbf{Semi-supervised:}
As a compromise of the two extremes, semi-supervised techniques operate with a mix of labeled and unlabeled data. This way it seeks to combine the low effort for labeling with the advantages of supervised learning \cite{ZhuEtAl2009}.
\end{itemize}

Regardless of their category, all learning methods make assumptions about the distribution of the data: feature vectors close to each other tend to belong to the same class and, consequently, data points are likely to form group-like structures.

\textbf{Active learning} is the name of a subgroup of semi-supervised methods which will be a center point of this work. To reduce the amount of labeled data needed, they choose one or more data points to be labeled, commonly with the intent to incrementally improve the built classifier's performance by adding more labeled data every iteration. Various methods to select the next data point(s) to be labeled exist; we will briefly introduce two of them: \textit{Uncertainty sampling} and \textit{Probabilistic Active Learning (PAL)}. The \textit{uncertainty} of a classifier with regard to a data point describes how unsure it is about its assigned class label. This can be either seen as the distance to a \textit{decision boundary}, that is the entity separating data points of different classes \cite{SchefferEtAl2001}, or as the posterior probability estimation for the class assignment of your classifier \cite{ZhuEtAl2008}. The probabilistic approach doesn't rely on uncertainty, instead it maximizes the expected performance gain in all data point neighbourhoods \cite{KremplEtAl2014}. A more in-depth description is given in \ref{evaluation}.

\section{Generalization Performance}
When a learning algorithm is used to induce a classifier, an obvious question is how well said classifier performs or how well it approximates the target function $f(\vec{x})$. This can be used to select an algorithm when multiple are available or to simply get an estimate on how often the classifier will misclassify data.

To facilitate the calculations further down a closer look at the training set $X_T$ is helpful. As earlier stated, the individual instances are supposed to be independently and identically distributed, which means the set can be seen as a random variable. Now the probability of a specific set depends on the probabilities to draw each of the instances and, in case of supervised and semi-supervised learning, their associated labels. Due to their independence, we can write it as 
\begin{equation}
p(X_T) = p(\vec{x}_1, y_1) \cdot ... \cdot p(\vec{x}_n, y_n) = \prod_{i=1}^{n} p(\vec{x}_i, y_i)
\end{equation}
\cite{RodriguezEtAl2013}. A similar formula applies for unsupervised learning.

\subsection{(Expected) prediction error, training error and loss}
To effectively evaluate the performance of a classifier it is important to quantify when it is mistaken. The \textit{loss function} $L(f(\vec{x}), \hat{f}(\vec{x}))$ describes the error a classifier makes for a specific feature vector. A popular loss function, especially for two-class problems, is \textit{0-1-loss}:
\begin{equation}
L(f(\vec{x}), \hat{f}(\vec{x})) =
\begin{cases}
	0, & \text{if }f(\vec{x}) = \hat{f}(\vec{x}) \\
	1, & \text{else}
\end{cases}
\end{equation}
For regression problems squared or absolute error loss are more effective, since equal function values are improbable in a continuous value space. It seems intuitive to use the feature vectors already used for training again for the performance evaluation, especially since they are already labeled and with that $f(\vec{x})$ for them known. Utilizing the loss function from above the \textit{training error} on the training set $X_T = \{(\vec{x}_1, y_1), ..., (\vec{x}_n, y_n)\}$ is
\begin{equation}
Err_{T} = \frac{1}{N}\sum_{i=1}^{n} L(y_i, \hat{f}(\vec{x}_i))
\end{equation}
\cite{HastieEtAl2009}. Unfortunately, using the measure to judge a classifier produces a side effect. A learning algorithm which creates a complex classifier that perfectly classifies all training instances will yield a training error of zero. If used on different data points from the same data set, however, an increase in misclassifications will be noted, likely more than for a less complex classifier with a few misclassifications on training data. This phenomenon is known as \textit{overfitting}, resulting from the memorization of $X_T$ by the classifier while a generalization onto the whole data set was wanted \cite{Dietterich1995}.

A more general error measure is the \textit{prediction error}. It draws independent samples from the data distribution $(\mathbf{X}, \mathbf{Y})$ and uses these to examine the loss of a classifier by calculating the expected value over all possible realizations:
\begin{equation}
Err_{S} = E_{(\mathbf{X}, \mathbf{Y})}[L(\mathbf{Y}, \hat{f}(\mathbf{X})) | X_T]
\end{equation}
\cite{RodriguezEtAl2013}. This approach bears a problem: the distribution of our data is usually unknown, hence the need for a classifier. In general, the prediction error cannot be computed directly, thus the need for estimators arises.
An important aspect of the prediction error is the dependence from the fixed training set. This allows for the performance estimation of an already trained classifier. A different approach takes away that dependence and instead calculates the expected error for all possible training sets $Err_{E} = E_{X_T}[Err_{S}]$, which equals
\begin{equation}
Err_{E} = E_{(\mathbf{X}, \mathbf{Y})}[L(\mathbf{Y}, \hat{f}(\mathbf{X}))]
\end{equation}
Here the performance of the algorithm that creates the model $\hat{f}(\vec{x})$ is evaluated, which is no longer of use for the analysis of a specific classifier but can guide the selection of a preferable algorithm. Note that we still require knowledge of the underlying distributions for a direct evaluation, meaning that realistically an estimator is needed for this as well \cite{HastieEtAl2009}.

\subsection{Bias and variance}

\subsection{Classificator-based estimators}

\subsection{Cross-Validation}

\subsection{Bootstrapping}

\section{Learning Curves and Regression Models}

\subsection{Function families}

\subsection{Algorithms}