\ifgerman{\chapter{Grundlagen und verwandte Arbeit}}{\chapter{Background and Related Work}}
\label{background}

%- Allgemeine Wissensgrundlagen des Fachgebiets
%- Spezielle Grundlagen, die für das Verständnis erforderlich sind
%- Rahmenbedingungen für die Arbeit
%- Ausführungen zum Stand des Wissens / der Technik
%Als Leitprinzip gilt: Nur Informationen erwähnen, die
%- später benötigt werden,
%- notwendig sind, um die Arbeit oder ihre Motivation zu verstehen
%Das heißt insbesondere,
%- keine Inhalte aus Lehrbüchern, außer
%- diese werden benötigt, um Problemstellung oder Lösungsweg zu definieren.

The main focus of our work is the estimation of classifier performance in the special case of an active learning scenario. To help better understand the problem setting, we will give a brief overview of the concepts of active learning as well as an in-depth look of existing methods for said estimation. We also establish parts of the notation that will be used for the remainder of this work.

\section{Classification and Active Learning}
Many problems to be solved encompass the differentiation between certain "classes" to which their inputs can be assigned. To explain the concept of \textbf{classification} we assume the example of a thermostat. Its purpose is to monitor the temperature in a certain area and fire up a heating unit to raise the temperature if necessary. But for that to happen, the thermostat has to \textit{classify} the measured temperature in the categories "too low" and "warm enough". In this special case a simple threshold usually suffices. Generally, however, a \textit{classifier} $C$ is defined as a mapping of a \textit{feature vector}, also called an \textit{instance}, to its corresponding \textit{class label} $y \in Y = \{y_1, ..., y_m\}$. It is convenient to describe said vector as an n-tuple of \textit{feature values}: $\vec{x} = (f_{a_1,1}, ..., f_{a_n, n})$ with $f_{i,j} \in F_j$ $\forall i = \{1, ..., |F_j|\}, j = \{1, ..., n\}$ and $F_j$ as a \textit{feature}. The goal of a classifier is to approximate the underlying "true" association of a feature vector to its class label $f(\vec{x}) = y$, which can be written as
\begin{equation}
C: F_1 \times ... \times F_n \rightarrow Y
\end{equation}
or $\hat{f}(\vec{x}) = \hat{y}$ \cite{RodriguezEtAl2013}. This definition makes it clear that classification is not restricted to single-variable problems. To stick with our example of a thermostat, you may want to consider the humidity, temperature outdoors or whether any windows are open to decide if heating is necessary. Closely related are \textit{regression problems}. While a classifier works with discrete class labels, regression uses a continuous output space.

To obtain a classifier for a specific problem, one makes use of what is called a \textit{learning algorithm} $A$. In a process called \textit{training} a set of instances, the \textit{training data} $X_T = \{(\vec{x}_1, y_1), ..., (\vec{x}_n, y_n)\}$, is taken and such a mapping $C$ using an optimization criterion is created. The training set is a subset of a distribution of all possible data and should be independently and identically distributed. The whole data set can be modeled as a bivariate distribution with the feature vectors and class labels as random variables: $(\mathbf{X}, \mathbf{Y})$ \cite{RodriguezEtAl2013}. While all learning algorithms are being fed feature vectors to create a classifier, they can be separated into three categories, depending on their requirement for labeling:
\begin{itemize}
\item \textbf{Supervised:}
This type of algorithm requires all of their input data to be labeled as well as a list of all possible class labels. While it may seem like the best option, its drawbacks include the potential cost of the labeling. If the correct class labels are not inherently known, usually a human is required to assign the correct labels manually. With unlabeled data readily available for problems like speech recognition, this is a very expensive part of the process. Another issue is the propagation of errors; any mistake made during labeling is carried over into the modeling of the classifier.

\item \textbf{Unsupervised:}
Instead of requiring all data to be labeled, unsupervised algorithms ignore class labels completely. This saves the cost of labeling the data, but many algorithms require some sort of tuning parameters (e.g. the cardinality of $Y$ and shape of underlying probability distribution). It is similar to the process of density estimation from statistics.

\item \textbf{Semi-supervised:}
As a compromise of the two extremes, semi-supervised techniques operate with a mix of labeled and unlabeled data. This way it seeks to combine the low effort for labeling with the advantages of supervised learning \cite{ZhuEtAl2009}.
\end{itemize}

Regardless of their category, all learning methods make assumptions about the distribution of the data: feature vectors close to each other tend to belong to the same class and, consequently, data points are likely to form group-like structures.

\textbf{Active learning} is the name of a subgroup of semi-supervised methods which will be a center point of this work. To reduce the amount of labeled data needed, they choose one or more data points to be labeled, commonly with the intent to incrementally improve the built classifier's performance by adding more labeled data every iteration. Various methods to select the next data point(s) to be labeled exist; we will briefly introduce two of them: \textit{Uncertainty sampling} and \textit{Probabilistic Active Learning (PAL)}. The \textit{uncertainty} of a classifier with regard to a data point describes how unsure it is about its assigned class label. This can be either seen as the distance to a \textit{decision boundary}, that is the entity separating data points of different classes \cite{SchefferEtAl2001}, or as the posterior probability estimation for the class assignment of your classifier \cite{ZhuEtAl2008}. The probabilistic approach doesn't rely on uncertainty, instead it maximizes the expected performance gain in all data point neighbourhoods \cite{KremplEtAl2014}. A more in-depth description is given in \ref{evaluation}.

\section{Generalization Performance}
When a learning algorithm is used to induce a classifier, an obvious question is how well said classifier performs or how well it approximates the target function $f(\vec{x})$. This can be used to select an algorithm when multiple are available or to simply get an estimate on how often the classifier will misclassify data.

To facilitate the calculations further down a closer look at the training set $X_T$ is helpful. As earlier stated, the individual instances are supposed to be independently and identically distributed, which means the set can be seen as a random variable. Now the probability of a specific set depends on the probabilities to draw each of the instances and, in case of supervised and semi-supervised learning, their associated labels. Due to their independence, we can write it as 
\begin{equation}
p(X_T) = p(\vec{x}_1, y_1) \cdot ... \cdot p(\vec{x}_n, y_n) = \prod_{i=1}^{n} p(\vec{x}_i, y_i)
\end{equation}
\cite{RodriguezEtAl2013}. A similar formula applies for unsupervised learning.

\subsection{(Expected) prediction error, training error and loss}
To effectively evaluate the performance of a classifier it is important to quantify when it is mistaken. The \textit{loss function} $L(f(\vec{x}), \hat{f}(\vec{x}))$ describes the error a classifier makes for a specific feature vector. A popular loss function, especially for two-class problems, is \textit{0-1-loss}:
\begin{equation}
L(f(\vec{x}), \hat{f}(\vec{x})) =
\begin{cases}
	0, & \text{if }f(\vec{x}) = \hat{f}(\vec{x}) \\
	1, & \text{else}
\end{cases}
\end{equation}
For regression problems squared or absolute error loss are more effective, since equal function values are improbable in a continuous output space. It seems intuitive to use the feature vectors already used for training again for the performance evaluation, especially since they are already labeled and with that $f(\vec{x})$ for them known. Utilizing the loss function from above the \textit{training error} on the training set $X_T = \{(\vec{x}_1, y_1), ..., (\vec{x}_n, y_n)\}$ is
\begin{equation}
Err_{T} = \frac{1}{N}\sum_{i=1}^{n} L(y_i, \hat{f}(\vec{x}_i))
\end{equation}
\cite{HastieEtAl2009}. If used with the 0-1-loss function, $1 - Err$ is also known as \textit{accuracy}: $acc = 1 - err = \frac{|Correct Classifications|}{|Total Classifications|}$.

Unfortunately, using the measure to judge a classifier produces a side effect. A learning algorithm which creates a complex classifier that perfectly classifies all training instances will yield a training error of zero. If used on different data points from the same data set, however, an increase in misclassifications will be noted, likely more than for a less complex classifier with a few misclassifications on training data. This phenomenon is known as \textit{overfitting}, resulting from the memorization of $X_T$ by the classifier while a generalization onto the whole data set was wanted \cite{Dietterich1995}.

A more general error measure is the \textit{prediction error}. It draws independent samples from the data distribution $(\mathbf{X}, \mathbf{Y})$ and uses these to examine the loss of a classifier by calculating the expected value over all possible realizations:
\begin{equation}
Err_{S} = E_{(\mathbf{X}, \mathbf{Y})}[L(\mathbf{Y}, \hat{f}(\mathbf{X})) | X_T]
\end{equation}
\cite{RodriguezEtAl2013}. This approach bears a problem: the distribution of our data is usually unknown, hence the need for a classifier. In general, the prediction error cannot be computed directly, thus the need for estimators arises.
An important aspect of the prediction error is the dependence from the fixed training set. This allows for the performance estimation of an already trained classifier. A different approach takes away that dependence and instead calculates the expected error for all possible training sets $Err_{E} = E_{X_T}[Err_{S}]$, which equals
\begin{equation}
Err_{E} = E_{(\mathbf{X}, \mathbf{Y})}[L(\mathbf{Y}, \hat{f}(\mathbf{X}))]
\end{equation}
Here the performance of the algorithm that creates the model $\hat{f}(\vec{x})$ is evaluated, which is no longer of use for the analysis of a specific classifier but can guide the selection of a preferable algorithm. Note that we still require knowledge of the underlying distributions for a direct evaluation, realistically making an estimator necessary as well \cite{HastieEtAl2009}.

\subsection{Bias and variance}
In the previous sections we portrayed classifier as a simple mapping $\hat{f}$ of input values $\vec{x}$ to class labels $y$. While this is true for many of them like decision trees, a more general point of view is to see a classifier as an assignment of probabilities for the class labels. Given our random variable $\mathbf{Y}$ for the class labels and a fixed value $\vec{x}$ of our input variable $\mathbf{X}$, $P(\mathbf{Y} = \hat{y} | \vec{x})$ represents the probability that $\mathbf{Y}$ realizes as the value $y$ given our input. For classifiers of the previously described type, only one class label has a non-zero probability for an input, leading to the possibility of being written as a function. In fact, since in practice a definite class label is needed, most classifiers will pick the class label which maximizes said probability anyway \cite{KohaviEtAl1996}.

Another assumption that doesn't necessarily hold was that an error-free target function $f(\vec{x})$ exists. It is entirely possible for our target function to be \textit{noisy}, that is to randomly vary from its true value. In turn, this \textit{variance} leads to blurry class assignments. Thus, we only get a probability $P(\mathbf{Y_T} = y | \vec{x})$ instead of a sharp mapping. Note that the class variable here is different from the one in the previous paragraph; they are conditionally independent for our target distribution and a fixed. Now with this probabilistic notation, it is possible to \textit{decompose} the expected prediction error from the previous section into three components:
\begin{equation}
\begin{split}
Err_{E} &= \sum_{\vec{x}}^{}P(\vec{x})\left(\sigma^2(\vec{x}) + bias^2(\vec{x}) + variance(\vec{x})\right) \\
&= \frac{1}{|X|}\sum_{\vec{x}}^{}\left(\sigma^2(\vec{x}) + bias^2(\vec{x}) + variance(\vec{x})\right)
\end{split}
\end{equation}
The bias and variance express by how much our classifier differs systematically and at random for a given data point from the true value. $\sigma^2$ is the variance of the noise distribution that the target may or may not have; it is the irreducible error that any classifier will always make. The need for the probability of $\vec{x}$ is dropped under the assumption of equal likelihood for all data points \cite{KohaviEtAl1996}. Ideally you would want to minimize both the bias and the variance of your classifier to achieve a low prediction error. Unfortunately, as the model complexity increases to accommodate for more subtle structures in the training data its bias will decrease but the variance will increase. This dilemma is known as the \textit{bias-variance-tradeoff} \cite{KroghVedelsby1995}.

\subsection{Classificator-based estimators}
As stated section 2.2.1, the training error of a classifier usually underestimates the true prediction error $Err_S$. To calculate the desired error, two options are available. Either a direct estimate is taken, usually with dedicated test sets, which will be called \textit{out-of-sample error} since it uses different data points than the training error, or the difference between training and true error is estimated and then added to the training error, also called \textit{optimism}. This section briefly introduces three methods to estimate the optimism: \textit{Akaike information criterion} and \textit{Bayes information criterion}.

The \textbf{Akaike information criterion}(AIC) in its original form was introduced by \cite{Akaike1998} in 1973. It is defined as a function of a model's likelihood and complexity $\lambda$: $AIC = -2 \cdot log(likelihood) + 2\lambda$ \cite{Bozdogan1987}. If instead of a 0-1-loss the log-likelihood-loss is used, a quantity that uses the logarithm of the likelihood of a model given the input, and a linear model with $\lambda$ parameters assumed, the AIC can also be written as
\begin{equation}
AIC = Err_T + 2\frac{\lambda}{n}\sigma^2
\end{equation}
with $n$ as the number of training instances and $\sigma^2$ the variance of the target model \cite{HastieEtAl2009}.

A very similar estimate is the \textbf{Bayesian information criterion}(BIC). Presented by \cite{Schwarz1978} as an alternative to AIC, its definition is similar: $BIC = -2 \cdot log(likelihood) + \lambda log(n)$. It is asymptotically equal to AIC for the size of the training set, but with a steeper penalty for complex models due to the exchange of the factor 2 with $log(n)$ \cite{Weakliem1999}. Using the same assumptions as for AIC, it can be calculated as
\begin{equation}
BIC = \frac{n}{\sigma^2}[Err_T + log(n) \cdot \frac{d}{N}\sigma^2]
\end{equation}

While both criteria are theoretically solid, they are impractical for use in this work; both assume fairly simple model families and restrict the use of loss functions \cite{HastieEtAl2009}.

The third estimator for the optimism is based on the \textbf{Vapnik-Chervonenkis(VC) theory}. In their publication \cite{Vapnik1982} they introduce an algorithm-specific quantity called VC-dimension which is defined as the number of data points a classifier can separate, regardless of their position and class label. Based on this, they derived an upper bound for the optimism of the training error:
\begin{equation}
sup|Err_S - Err_T| \leq 2\frac{ln(\frac{2|X_T|}{h})}{l / h}
\end{equation}
with $h$ as the VC-dimension. The derived bound has some restrictions, however; it is only valid for large training sets and requires knowledge of the VC-dimension. Unfortunately, analytical solutions are known only for a few algorithms and it is not given that it is constant with regard to the training set size \cite{BrumenEtal2004}.

\subsection{Cross-Validation}
As the training error overestimates the classifier performance due to the same samples from the data being used for training as well as evaluation, a different approach is to use separate data for testing. Using this data set called \textit{hold-out set} gives a direct estimate of the prediction error. A common split is to use two thirds of the available (labeled) data as training data and the rest for testing. However, since labeling data can be expensive, often only little labeled data is available. In that case, holding out a third of it may significantly reduce the classifier's performance. An alternative approach is known as \textit{cross-validation} in which the classifier is trained with the full training set. For the performance evaluation the classifier is then retrained with a part of the data, the rest is used for testing \cite{Kohavi1995}.

\textbf{K-fold cross-validation} is the most simple form of cross-validation. Given a set of labeled data $X_T$, $k$ new sets $X_k$ are created, each with size $n$. For each of the $k$ sets a classifier is trained with $X_T \setminus X_k$. $X_k$ then serves as the test set on which the classifier is evaluated, resulting in $k$ estimations of the performance to be expected of the classifier trained with the whole set $X_T$:
\begin{equation}
\widehat{Err}_{S,i} = \frac{1}{n} \sum_{j=1}^{n} L(y_{i, j}, \hat{f}(\vec{x}_{i, j}))
\end{equation}
with $i = \{1, ..., k\}$ \cite{Kohavi1995}. These estimations can be seen as samples of a performance distribution; examples can be uniform (simple average) or beta distribution \cite{KremplEtAl2014}. As a special case of k-fold cross-validation, \textbf{leave-one-out} cross-validation works with test sets of size one. To ensure similarity to the true data distribution, the folds are usually \textit{stratified}; this way the ratio of class labels in $X_T$ is kept also in the test sets.

More expensive variants are \textbf{complete} and \textbf{leave-pair-out} cross-validation. The former uses every possible two-set split of $X_T$ as basis of the estimation, while the latter only regards the possible pairs of data points. Complete cross-validation is rarely used due to its computational complexity \cite{Kohavi1995}, while leave-pair-out seems to strike an acceptable mix of effort and accuracy \cite{PahikkalaEtAl2008}.

An algorithm specifically designed to work with iterative labeling was introduced by \cite{BrumenEtal2004} under the name of \textit{adaptive incremental k-fold cross validation}. It performs k-fold cross-validation after each increase of the training set size and stops when a certain performance threshold is reached. \cite{AirolaEtAl2001} perform an empirical study of the behaviour for different cross-validation techniques as well as \textit{bootstrapping}, which will be described in the next section. Their findings indicate leave-pair-out and k-fold with $k = 5$ or $10$ with averaged results as the most robust approaches. They also find them to be unbiased performance estimators, which is also stated in \cite{Kohavi1995}. \cite{RodriguezEtAl2013} clarifies that this only holds true for the performance estimation of classifier trained with $n - \frac{n}{k}$ instances. The estimation of classifier performance trained with $n$ samples is afflicted with a positive bias, overestimating the error rate. \cite{EfronEtAl1997}  reinforce this argument with experiments and provide assessment for the variance as well.

\subsection{Bootstrapping}
While closely related to cross-validation, \textbf{bootstrap} takes a slightly different approach. Instead of splitting $X_T$ into mutually exclusive sets, $b$ sets of equal size $n$ are created and filled by random sampling of elements from $X_T$. These are sampled with replacement, meaning can be drawn multiple times into the same set. The classifier is then trained on every bootstrap set $X^b_i$ and tested against the original set $X_T$:
\begin{equation}
\widehat{Err}^{boot}_i = \frac{1}{|X_T|} \sum_{j=1}^{|X_T|} L(y_j, \hat{f}^b_i(\vec{x}_j))
\end{equation}
\cite{Kohavi1995}.

Using the whole set $X_T$ as a test set, however, has some unwanted side effects. Because the bootstrap set used to train the classifier and $X_T$ overlap, the classifier's performance is estimated with samples it has already seen in training, leading to a negative bias for the estimated error rate. To avoid this, the bootstrap trained classifiers are only tested on samples which are not part of their training set:
\begin{equation}
\widehat{Err}^{LOO}_i = \frac{1}{|X_T \setminus X^b_i|} \sum_{(\vec{x}, y) \in X_T \setminus X^b_i}^{} L(y, \hat{f}^b_i(\vec{x}))
\end{equation}
, a technique known as \textbf{leave-one-out bootstrapping} or \textbf{LOO} \cite{BorraEtAl2010}.

A different bootstrap estimator was introduced by \cite{Efron1983}. While vanilla bootstrap is biased downwards with regard to the estimated error rate, its leave-one-out version goes over the top. The reason for this is similar to the problems of cross-validation: Since the classifier is trained with less instances than available for the estimation, it lacks knowledge that a classifier learned on the full training set has, leading to a worse prediction performance and thus underestimating the performance. As proven by Efron in the same publication, the fraction of unique instances from $X_T$ in a bootstrap set is expected to be $1 - e^{-1} \approx 0.632$. By seeing bootstrapping actually as an estimator for the optimism, similar to AIC and BIC, he derives the so called \textbf{.632 bootstrap}:
\begin{equation}
\widehat{Err}^{.632} = 0.368 \cdot Err_T + 0.632 \cdot \frac{1}{b} \sum_{i=1}^{b} \widehat{Err}^{LOO}_i
\end{equation}
It factors in the training error to lift up the otherwise too conservative estimate of the performance \cite{Efron1983}.

Building on that, \cite{EfronEtAl1997} developed a modified .632 bootstrap under the namse of \textbf{.632+ bootstrap}. As their experiments showed, the .632 method has problems estimating the performance of an extremely overfit classifier. To account for this, they use two the two quantities \textit{no-information error rate} and \textit{relative overfitting}. The no-information error rate $\hat{\gamma}$ is defined as as the error rate of a classifier when no dependence exists between input and output data, i.e. $\mathbf{X}$ and $\mathbf{Y}$ are independent. Relative overfitting $\hat{R}$ then expresses how close the classifier is overfit to the no-information error rate:
\begin{equation}
\hat{R} =
\begin{cases}
	\frac{\widehat{Err}^{LOO} - Err_T}{\hat{\gamma} - Err_T}, & \text{if } \widehat{Err}^{LOO}, \hat{\gamma} > Err_T \\
	0, & \text{else}
\end{cases}
\end{equation}
The .632+ error rate is then defined as
\begin{equation}
\widehat{Err}^{.632+} = \widehat{Err}^{.632} + \left(min(\widehat{Err}, \hat{\gamma}) - Err_T\right) \cdot \frac{0.368 \cdot 0.632 \cdot \hat{R}}{1 - 0.368 \cdot \hat{R}}
\end{equation}
Part of the assumptions behind this estimator is that leave-one-out bootstrapping has the correct expected error rate value in the case of independence of $\mathbf{X}$ and $\mathbf{Y}$ and a class probability of 0.5 in a 2-class problem, whereas the .632 does not \cite{EfronEtAl1997}. However, \cite{WoodEtAl2007} show in their paper about different error rate estimators that leave-one-out slightly underestimates the error. Unfortunately, no experiments with .632+ were performed to evaluate the influence of this.

\section{Learning Curves and Regression Models}
The error measures described in section 2.2.1 give an (theoretically) accurate idea about a classifier's performance for either a specific or all possible training sets. In the case of, for instance, active learning, the learning algorithm isn't just fed a single training set. Instead, instances are added iteratively and a new classifier gets trained each round. For this, a representation of the algorithms progress in terms of the resulting classifiers' error rates seems desirable. Originating in the field of psychology, it describes a collection of models to assess the level of comprehension in an individual over the time of exposure or number of examples \cite{Yelle1979}. Similarly, in the context of machine learning it describes a function mapping the size of the training set to a measure of performance; a commonly used measure is accuracy \cite{PerlichEtAl2003}.

As a known functional dependency between accuracy and training set size would enable an easy lookup of the to-be-expected error rate for a certain amount of labeled data, attempts have been made to find a fitting function model.

\subsection{Function families}
The typical form of a learning curve is a steep increase in accuracy when few examples have been presented, leveling more and more for an increased training set, converging towards a maximal accuracy \cite{FigueroaEtal2012}. An example of such a curve can be seen in figure \ref{fig:curve_general}.
\begin{figure}[h]
	\includegraphics[width = .8\textwidth]{learning_curve_general.png}
	\caption{General appearance of a learning curve. Training set size as x-axis, accuracy as y-axis \cite{FigueroaEtal2012}}
	\label{fig:curve_general}
\end{figure}
Functions families fitting this description are \textbf{power} \cite{FigueroaEtal2012,Singh2005}, \textbf{logarithmic} and \textbf{exponential} \cite{Singh2005}. These papers provide an overview for the fitting of the function types. \cite{Singh2005} uses linear fitting with least squared error as the target and the resulting correlation coefficient as a measure of fitting for four different function models and four classifiers. The power law $f(x) = a + b \cdot x^c$, with $a, b, c$ being regression parameters, performed best on most data sets tested and is also the basis for a proposed method of performance prediction in \cite{FigueroaEtal2012}. It has to be noted, though, that in a fourth of all cases outperforms a linear model the more complex models, mostly on the same data set. This may hint at a data set dependency for the correct curve model.

A related approach is described in \cite{CortesEtal1993}. Here, instead of modeling the prediction error directly, an exponential model is assumed for the optimism of the training error: $Err_S - Err_T = \frac{2b}{|X_T|^\alpha}$ and $Err_S + Err_T = 2a$. Using already acquired (true) error rates, the parameters are estimated from the gradient and amplitude of the data after being transformed into a logarithmic scale and fitted linearly. Extrapolating the function then gives an estimate of the expected error rate for a given $|X_T|$.

\section{Miscellaneous}
Introduced in \cite{EvansEtAl2015}, \textbf{Model Retraining Improvement} is originally intended as a statistically optimal criteria for instance selection in active learning. However, it does so by performing an unbiased estimate of the loss improvement a new labeled instance would yield. While the algorithm itself does not provide a way to estimate the future loss (or current, either), the paper provides a theoretical background for potential future estimators.

Similar to the extrapolation of fitted learning curves is the \textit{maximum-likelihood-regression} presented in \cite{KadieEtal1995}. While the former has a fixed model to regress on, their method finds the most probable model for the given data.

\cite{RoyEtAl2001} also presents a method to estimate current and future loss of a classifier in an active learning setting, similarly to model retraining improvement. They, however, use \textit{Monte-Carlo-Sampling}, resulting in a process comparable to the techniques already discussed in this chapter.