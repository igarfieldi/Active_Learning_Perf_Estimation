\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{FigueroaEtal2012}
\citation{KremplEtAl2014}
\citation{GuptaEtAl2004}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Evaluation}{23}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{evaluation}{{Chapter~4}{23}{Evaluation}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Objective and measurements}{23}{section.4.1}}
\@writefile{brf}{\backcite{FigueroaEtal2012}{{23}{Section~4.1}{section.4.1}}}
\citation{KullbackEtAl1951}
\citation{Joyce2011}
\@writefile{brf}{\backcite{KremplEtAl2014}{{24}{Section~4.1}{equation.4.1.2}}}
\@writefile{brf}{\backcite{GuptaEtAl2004}{{24}{Section~4.1}{equation.4.1.2}}}
\@writefile{brf}{\backcite{KullbackEtAl1951}{{24}{Section~4.1}{equation.4.1.5}}}
\@writefile{brf}{\backcite{Joyce2011}{{24}{Section~4.1}{equation.4.1.5}}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Method selection}{24}{section.4.2}}
\citation{Dietterich1995}
\citation{FigueroaEtal2012}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Test environment}{25}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Function models}{25}{subsection.4.3.1}}
\@writefile{brf}{\backcite{Dietterich1995}{{25}{Section~4.3.1}{subsection.4.3.1}}}
\@writefile{brf}{\backcite{FigueroaEtal2012}{{25}{Section~4.3.1}{subsection.4.3.1}}}
\citation{ZhuEtAl2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Active learner}{26}{subsection.4.3.2}}
\@writefile{brf}{\backcite{ZhuEtAl2008}{{26}{Section~4.3.2}{subsection.4.3.2}}}
\citation{SheatherEtAl1991}
\citation{Silverman1986}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces PAL illustration showing the weighted probabilistic gain (normed on $[0, 1]$). Since all labeled instances are grouped by their class label, the gain is higher for near the lower label-density group}}{27}{figure.4.1}}
\newlabel{fig:PALIllustration}{{Figure~4.1}{27}{PAL illustration showing the weighted probabilistic gain (normed on $[0, 1]$). Since all labeled instances are grouped by their class label, the gain is higher for near the lower label-density group}{figure.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Classifier}{27}{subsection.4.3.3}}
\newlabel{evaluation:classifier}{{Section~4.3.3}{27}{Classifier}{subsection.4.3.3}{}}
\newlabel{equ:uniKDE}{{Equation~4.7}{27}{Classifier}{equation.4.3.7}{}}
\citation{ArchambeauEtAl2006}
\@writefile{brf}{\backcite{SheatherEtAl1991}{{28}{Section~4.3.3}{equation.4.3.7}}}
\@writefile{brf}{\backcite{Silverman1986}{{28}{Section~4.3.3}{equation.4.3.7}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The estimated kernel density for a grid with one positive and two negative instance; lower Z value indicates lower certainty for the class assignment}}{28}{figure.4.2}}
\newlabel{fig:KDE3inst}{{Figure~4.2}{28}{The estimated kernel density for a grid with one positive and two negative instance; lower Z value indicates lower certainty for the class assignment}{figure.4.2}{}}
\@writefile{brf}{\backcite{ArchambeauEtAl2006}{{28}{Section~4.3.3}{figure.4.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Datasets}{28}{subsection.4.3.4}}
\citation{Chapelle2005}
\citation{KremplEtAl2014}
\citation{CharytanowiczEtAl2010}
\citation{vanDerMaaten2008}
\citation{NashEtAl1994}
\citation{Chapelle2005,KremplEtAl2014,CharytanowiczEtAl2010,NashEtAl1994}
\citation{vanDerMaaten2008}
\citation{Chapelle2005,KremplEtAl2014,CharytanowiczEtAl2010,NashEtAl1994}
\citation{vanDerMaaten2008}
\@writefile{brf}{\backcite{Chapelle2005}{{29}{Section~4.3.4}{subsection.4.3.4}}}
\@writefile{brf}{\backcite{KremplEtAl2014}{{29}{Section~4.3.4}{subsection.4.3.4}}}
\@writefile{brf}{\backcite{CharytanowiczEtAl2010}{{29}{Section~4.3.4}{subsection.4.3.4}}}
\@writefile{brf}{\backcite{vanDerMaaten2008}{{29}{Section~4.3.4}{subsection.4.3.4}}}
\@writefile{brf}{\backcite{NashEtAl1994}{{29}{Section~4.3.4}{subsection.4.3.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Visualizations of the datasets checke1, 2dData, seeds and a downsized version of abalone \cite  {Chapelle2005,KremplEtAl2014,CharytanowiczEtAl2010,NashEtAl1994}.\newline  The illustration of seeds and abalone was done using an implementation of t-SNE \cite  {vanDerMaaten2008}}}{30}{figure.4.3}}
\@writefile{brf}{\backcite{Chapelle2005}{{30}{Figure~4.3}{figure.4.3}}}
\@writefile{brf}{\backcite{CharytanowiczEtAl2010}{{30}{Figure~4.3}{figure.4.3}}}
\@writefile{brf}{\backcite{KremplEtAl2014}{{30}{Figure~4.3}{figure.4.3}}}
\@writefile{brf}{\backcite{NashEtAl1994}{{30}{Figure~4.3}{figure.4.3}}}
\@writefile{brf}{\backcite{vanDerMaaten2008}{{30}{Figure~4.3}{figure.4.3}}}
\newlabel{fig:datasetIllustrations}{{Figure~4.3}{30}{Visualizations of the datasets checke1, 2dData, seeds and a downsized version of abalone \cite {Chapelle2005,KremplEtAl2014,CharytanowiczEtAl2010,NashEtAl1994}.\newline The illustration of seeds and abalone was done using an implementation of t-SNE \cite {vanDerMaaten2008}}{figure.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Parameters}{30}{subsection.4.3.5}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Function-specific parameters for model fitting}}{31}{table.4.1}}
\newlabel{tab:functionParams}{{Table~4.1}{31}{Function-specific parameters for model fitting}{table.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Test results}{31}{section.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Average Mean Error}{31}{subsection.4.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Average mean errors for the different active learners and datasets using the exponential model. The darker colors of a bar mark the errors of later learning stages (bright -> dark: $[3,7]$, $[8,15]$, $[16,30]$ training set size)}}{32}{figure.4.4}}
\newlabel{fig:meanErrorsExp}{{Figure~4.4}{32}{Average mean errors for the different active learners and datasets using the exponential model. The darker colors of a bar mark the errors of later learning stages (bright -> dark: $[3,7]$, $[8,15]$, $[16,30]$ training set size)}{figure.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Average mean errors for the different active learners and datasets using the sigmoid model. The darker colors of a bar mark the errors of later learning stages (bright -> dark: $[3,7]$, $[8,15]$, $[16,30]$ training set size)}}{33}{figure.4.5}}
\newlabel{fig:meanErrorsSig}{{Figure~4.5}{33}{Average mean errors for the different active learners and datasets using the sigmoid model. The darker colors of a bar mark the errors of later learning stages (bright -> dark: $[3,7]$, $[8,15]$, $[16,30]$ training set size)}{figure.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Average mean errors for the different active learners and datasets using the linear model. The darker colors of a bar mark the errors of later learning stages (bright -> dark: $[3,7]$, $[8,15]$, $[16,30]$ training set size)}}{34}{figure.4.6}}
\newlabel{fig:meanErrorsLin}{{Figure~4.6}{34}{Average mean errors for the different active learners and datasets using the linear model. The darker colors of a bar mark the errors of later learning stages (bright -> dark: $[3,7]$, $[8,15]$, $[16,30]$ training set size)}{figure.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Average squared error}{35}{subsection.4.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Average mean errors for different datasets with random sampling. The darker colors of a bar mark the errors of later learning stages (bright -> dark: $[3,7]$, $[8,15]$, $[16,30]$ training set size)}}{36}{figure.4.7}}
\newlabel{fig:meanErrorsWeighted}{{Figure~4.7}{36}{Average mean errors for different datasets with random sampling. The darker colors of a bar mark the errors of later learning stages (bright -> dark: $[3,7]$, $[8,15]$, $[16,30]$ training set size)}{figure.4.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Average squared errors for the different active learners, function models and datasets}}{37}{figure.4.8}}
\newlabel{fig:squaredErrors}{{Figure~4.8}{37}{Average squared errors for the different active learners, function models and datasets}{figure.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Kullback-Leibler divergence}{37}{subsection.4.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Computation Time}{37}{subsection.4.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Kernel density estimations from the learning process of PAL for checke1 at 3, 8, 16 and 21 training instances}}{38}{figure.4.9}}
\newlabel{fig:PALKDEs}{{Figure~4.9}{38}{Kernel density estimations from the learning process of PAL for checke1 at 3, 8, 16 and 21 training instances}{figure.4.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Average Kullback-Leibler divergence for selected methods}}{39}{figure.4.10}}
\newlabel{fig:klDiv}{{Figure~4.10}{39}{Average Kullback-Leibler divergence for selected methods}{figure.4.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Left: Average computation times for the estimators. Right: Histogram of computation time for pathSuper}}{39}{figure.4.11}}
\newlabel{fig:compTimeAll}{{Figure~4.11}{39}{Left: Average computation times for the estimators. Right: Histogram of computation time for pathSuper}{figure.4.11}{}}
\@setckpt{chapters/evaluation}{
\setcounter{page}{40}
\setcounter{equation}{7}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{11}
\setcounter{table}{1}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{6}
\setcounter{NAT@ctr}{0}
\setcounter{vrcnt}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{43}
\setcounter{su@anzahl}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{definition}{0}
\setcounter{theorem}{0}
\setcounter{lemma}{0}
\setcounter{ALG@line}{14}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{float@type}{16}
\setcounter{algorithm}{3}
\setcounter{section@level}{2}
\setcounter{lstlisting}{0}
}
