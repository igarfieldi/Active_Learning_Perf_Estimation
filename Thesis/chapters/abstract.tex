\chapter*{Inhaltsangabe}

Im Bereich des \textit{maschinellen Lernens} werden Klassifikatoren verwendet, um von Daten mit bekannter auf solche mit unbekannter Klassenzuordnung zu schließen. Da das Erlangen dieser Zuordnungen mitunter aufwändig ist, wird die Auswahl geeigneter Daten von \textit{Active Learnern} durchgeführt. Um zu beurteilen, ob ausreichend bekannte Daten für eine bestimmte Genauigkeit der Zuordnung vorliegen, muss diese geschätzt werden.

In dieser Arbeit wird die Fragestellung beleuchtet, ob eine Kombination von Kurvenregression und Kreuzvalidierung die Leistung eines Klassifikators bei Benutzung verschiedener aktiver Lerner ohne systematische Abweichungen schätzen kann, besonders wenn noch nicht viele Trainingsinstanzen vorliegen. Zu diesem Zweck untersuche ich die systematischen Schätzfehler von vier verschiedene Verfahren auf Basis von bestehenden, Daten-basierten Verfahren zur Leistungsschätzung unter Verwendung von drei Funktionsmodellen sowie verschiedene Varianten. Mit in Betracht gezogen werden auch die Streuung der Schätzungen sowie die benötigte Rechenzeit. In simulierten Tests werden die Kombinationen für verschiedene Datensätze und Active Learner mit bereits erprobten Schätzern verglichen.

Die Resultate zeigen eine starke Abhängigkeit des Schätzfehlers vom gewählten Active Learner, wobei nur für zufälliges Ziehen akzeptable Fehler auftreten. Außerdem weißt eine der Methoden einen geringeren Schätzfehler als die bekannten Schätzer für kleine Trainingsmengen auf, allerdings zu Lasten eines hohem Rechenaufwands.


\chapter*{Abstract}

In the field of \textit{machine learning}, classifiers are used to predict a class assignment of unknown data based on known instances. Since obtaining already labeled data may be expensive, \textit{active learners} are used to select adequate data. To assess whether more labeled data is needed to achieve a certain classification accuracy, performance estimation is needed.

In this work we evaluate the question if a combination of curve fitting and cross-validation produces an estimator without systematic error capable of assessing a classifier's performance, especially with few purchased training instances. For this purpose, I examine the estimation bias of four methods based on existing data-based performance estimation techniques making use of three function models as well as variations. Also of interest are the spread of the estimations as well as the necessary computation time. The methods are tested and compared against state-of-the-art estimators for different datasets and active learners.

The results show a strong dependency between an estimators error and the chosen active learner with acceptable errors only for random sampling. Additionally, one method shows a lower bias than the established estimators for small training sets. This, however, comes at the cost of high computational effort.