\ifgerman{\chapter{Einf√ºhrung}}{\chapter{Introduction}}

The field of machine learning has changed quite a bit in the past couple of decades. While it was difficult in the past to gather enough data on some characteristic to create reliable automatic classification, todays processing speed and memory capacities as well as the global connectivity via the Internet allow for large databases available for use. However, often only the part of the data which will be used to predict the characteristic is abundant, which is called \textit{features}. The other part, in case of a discrete characteristic going by the name \textit{class label}, has to be assigned by an annotator. This can result in errors and be costly, especially if the annotator is a human. Using the field of speech recognition as an example, spoken words are easy to obtain from videos, radio or phone calls, what words exactly were spoken is usually unknown. To be able to use the data, a human would have to listen to it and write the correct words down, which is a boring and time-consuming, but necessary, work.

Due to this, a class of algorithms called \textit{active learners} has formed to help minimize the work necessary. As a rule of thumb, the more annotated data is used for the classification, the better it will perform, i.e. less misclassifications occur. But it is important which part of the data has been selected, as some instances, the combination of features and class label, are more informative as others. Thus, annotating the useful instances first and the rest later or not at all minimizes the impairment of the classifier's performance for equal effort. In order to stop the annotation when it is no longer sensible or wanted, we need some way of estimating the current performance. Traditionally, this is done one of three ways: either some part of the annotated data is set aside for so-called holdout testing, which is unwanted since it wastes precious annotated data, some kind of partitioning to create artificial test sets is done, or the classifier's performance in the past in conjuncture with a suitable function model is used to extrapolate to the current situation. The second option struggles with systematic deviation of their estimations from the true performance, while the third heavily depends on the model chosen as well as a number of already present estimates.

In this thesis I will investigate the properties of methods combining partitioning as well as information about the learning process. For this, subsets are created from the data used for training. These are then seen as individual, smaller training sets and build the basis of classifiers themselves, with the remaining data serving as a test set. This results in a number of performance estimations for training sets of various sizes, which can be seen as labeled instances for a different learning problem and are used to learn a model predicting the performance development of the original classifier. Applying the model to the current classifier state described by its training set size then results in a performance prediction. The goal is to evaluate the potential bias and error spread of this method family in comparison to the state-of-the-art estimators k-fold cross-validation and .632+ bootstrapping using different active learners, datasets and model types.

The next chapter gives an overview of classification and active learning in general and touches on currently existing ways of estimating the performance of a classifier. After that, chapter 3 explains the concept of the methods to be evaluated and its variations. After that follows the \textit{Evaluation}, first presenting the settings the testing and later its results. The end is marked by the conclusion, summarizing the results and providing an outlook for possible future work in this sector.