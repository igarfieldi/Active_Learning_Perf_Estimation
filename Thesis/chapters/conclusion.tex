\ifgerman{\chapter{Fazit und weiterf√ºhrende Arbeit}}{\chapter{Conclusion and Future Work}}
\label{conclusion}

In this thesis, the properties of four performance estimators utilizing estimation on training subsets, model fitting and subsequent extrapolation to approximate the accuracy of a classifier in the context of active learning. For this, the methods \textit{path} and \textit{pathSuper}, which simulating the classifier's training process, as well as \textit{averaged} and \textit{averagedBS}, which use leave-p-out cross-validation, were evaluated on different datasets and active learners with regard to their estimation bias, spread and computation time. Each of these estimators was tested with an exponential, sigmoid, and linear function model as basis for the extrapolation, broken down by learning stage. Additionally, the effect of enhancements to the fitting process for some selected estimators were studied. To compare them to state-of-the-art methods, \textit{5-fold cross-validation} and \textit{.632+ bootstrapping} also participated in the evaluation.

The results show that a distinction between random and non-random active learning has to be made. Some of the estimators tested are definitely viable for random sampling, namely \textit{averagedBS} with the linear, \textit{averaged} with the sigmoid and \textit{pathSuper} with the exponential model. This is especially true for a classifier trained with a set of instances in the size range of 3 to 7. While it is dependent on the dataset, they show lower biases than \textit{.632+ BS} and \textit{5-fold CV}, but slightly elevated estimation spread. For larger training sets, however, the traditional bootstrapping is the reference.

For both uncertainty sampling and PAL, none of the estimators, including traditional cross-validation and bootstrapping, are suitable. Depending on how well PAL can abuse the data structure, the estimates are more or less pessimistic. This is caused by holding out instances from the training set; it reduces the classifier's information too much, since the instance was likely to be the only one in the specific area. A similar problem has to be faced with uncertainty sampling: instances at decision boundaries tend to be noisy and of mixed labels; a classifier predicting a noisy instance's label is likely to be wrong, which lowers the accuracy estimate.

The addition of statistical weights to the model fitting process showed mixed results. Its effectiveness is largely dependent on the function model itself, the estimator and the learning stage. It reduces the bias of \textit{averagedBS} with the linear model for training set sizes between three and seven by $~20\%$. The bias of the same classifier for larger training sets rises, however. Another addition, an estimate for a completely untrained classifier with the name \textit{no-information rate}, only increased the bias in the tests.

All of the non-traditional estimators require heavy amounts of computing, caused by their exponential complexity. The computation time is mostly independent of the dataset, but varies largely due to the iterative nature of the model fitting.

For future work, I suggest to take a look at different weights for the fitting and why the function models are affected so differently. It may also be of interest to investigate how many estimates are used for the linear model; as it stands, those of the largest four subsets are the only ones. Less could better reflect the current accuracy gradient, but also cause more instability.

Also, the \textit{pathSuper} estimator simulates different possible classifier paths. Which instance is added to the simulated training set next is random with uniform distribution. Seeing as both uncertainty sampling and PAL carry a selection bias, taking these possible preferences for instances into account may lead to an improved estimation when using these active learners.

Another possibility is a hybrid estimator: for random sampling, \textit{averagedBS} with the linear model produces the least biased estimates for small training sets, while \textit{.632+ bootstrapping} does so for sizes larger than $7$. Figuring out the breaking points and using both for the corresponding training set size may be beneficial.