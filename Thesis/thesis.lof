\select@language {english}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces PAL illustration showing the weighted probabilistic gain (normed on $[0, 1]$). The pgain is highest in areas where no labels have been purchased yet; the present ones have been selected at random}}{6}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Classifier state for random and uncertainty sampling as well as PAL for 16 purchased labels on checke1}}{7}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces General appearance of a learning curve. Training set size as x-axis, accuracy as y-axis \cite {FigueroaEtal2012}}}{14}{figure.2.3}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Left: Three example paths generated with path-superset sub-sampling. The zigzag shape illustrates the difficulties for the fitting. Right: All possible accuracies for training subsets with $k = 6$}}{18}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces The shape of the three function models fit on example estimates produced by \textit {averaged grouping} with $k = 9$ for the \textit {seeds} dataset}}{22}{figure.3.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Difference of function shape for the sigmoid model when using weighting or the no-information rate}}{23}{figure.3.3}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces The estimated kernel density for a grid with one positive and two negative instances; lower Z value indicates lower certainty for the class assignment}}{30}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Visualizations of the datasets checke1, 2dData, seeds and a downsized version of abalone \cite {Chapelle2005,KremplEtAl2014,CharytanowiczEtAl2010,NashEtAl1994}.\newline The illustration of seeds and abalone was done using an implementation of t-SNE \cite {vanDerMaaten2008}}}{31}{figure.4.2}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Visualizations of the datasets checke1, 2dData, seeds and a downsized version of abalone \cite {Chapelle2005,KremplEtAl2014,CharytanowiczEtAl2010,NashEtAl1994}.\newline The illustration of seeds and abalone was done using an implementation of t-SNE \cite {vanDerMaaten2008}}}{32}{figure.4.3}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Average mean errors for the different active learners and datasets using the exponential model. The darker colored bars mark the errors of later learning stages}}{34}{figure.4.4}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Average mean errors for the different active learners and datasets using the sigmoid model}}{35}{figure.4.5}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Average mean errors for the different active learners and datasets using the linear model}}{37}{figure.4.6}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Average mean errors for different datasets with random sampling}}{38}{figure.4.7}
\contentsline {figure}{\numberline {4.8}{\ignorespaces Average squared errors for the different active learners and datasets with the respective share of each learning stage}}{39}{figure.4.8}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Average Kullback-Leibler divergence for selected methods}}{40}{figure.4.9}
\contentsline {figure}{\numberline {4.10}{\ignorespaces Left: Average computation times for the estimators. Right: Histogram of computation time for pathSuper}}{41}{figure.4.10}
\addvspace {10\p@ }
